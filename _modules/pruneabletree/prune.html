

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>pruneabletree.prune &mdash; vhsven-sklearn 0.1.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> vhsven-sklearn
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../api.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../auto_examples/index.html">General examples</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">vhsven-sklearn</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>pruneabletree.prune</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for pruneabletree.prune</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Module containing PruneableDecisionTreeClassifier class.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="k">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>

<span class="kn">from</span> <span class="nn">.pruner</span> <span class="k">import</span> <span class="n">Pruner</span>
<span class="kn">from</span> <span class="nn">.pruner_rep</span> <span class="k">import</span> <span class="n">ReducedErrorPruner</span>
<span class="kn">from</span> <span class="nn">.pruner_ebp</span> <span class="k">import</span> <span class="n">ErrorBasedPruner</span>

<div class="viewcode-block" id="PruneableDecisionTreeClassifier"><a class="viewcode-back" href="../../pruneabletree.prune.html#pruneabletree.prune.PruneableDecisionTreeClassifier">[docs]</a><span class="k">class</span> <span class="nc">PruneableDecisionTreeClassifier</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A pruneable decision tree classifier.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    criterion : string, optional (default=&quot;gini&quot;)</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria are</span>
<span class="sd">        &quot;gini&quot; for the Gini impurity and &quot;entropy&quot; for the information gain.</span>

<span class="sd">    splitter : string, optional (default=&quot;best&quot;)</span>
<span class="sd">        The strategy used to choose the split at each node. Supported</span>
<span class="sd">        strategies are &quot;best&quot; to choose the best split and &quot;random&quot; to choose</span>
<span class="sd">        the best random split.</span>

<span class="sd">    max_depth : int or None, optional (default=None)</span>
<span class="sd">        The maximum depth of the tree. If None, then nodes are expanded until</span>
<span class="sd">        all leaves are pure or until all leaves contain less than</span>
<span class="sd">        min_samples_split samples.</span>

<span class="sd">    min_samples_split : int, float, optional (default=2)</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>

<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_samples_leaf : int, float, optional (default=1)</span>
<span class="sd">        The minimum number of samples required to be at a leaf node:</span>

<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_weight_fraction_leaf : float, optional (default=0.)</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    max_features : int, float, string or None, optional (default=None)</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>

<span class="sd">            - If int, then consider `max_features` features at each split.</span>
<span class="sd">            - If float, then `max_features` is a fraction and</span>
<span class="sd">              `int(max_features * n_features)` features are considered at each</span>
<span class="sd">              split.</span>
<span class="sd">            - If &quot;auto&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">            - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">            - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">            - If None, then `max_features=n_features`.</span>

<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>

<span class="sd">    max_leaf_nodes : int or None, optional (default=None)</span>
<span class="sd">        Grow a tree with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    min_impurity_decrease : float, optional (default=0.)</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>

<span class="sd">        The weighted impurity decrease equation is the following::</span>

<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>

<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>

<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    min_impurity_split : float,</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>

<span class="sd">        .. deprecated:: 0.19</span>
<span class="sd">           ``min_impurity_split`` has been deprecated in favor of</span>
<span class="sd">           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.</span>
<span class="sd">           Use ``min_impurity_decrease`` instead.</span>

<span class="sd">    class_weight : dict, list of dicts, &quot;balanced&quot; or None, default=None</span>
<span class="sd">        Weights associated with classes in the form ``{class_label: weight}``.</span>
<span class="sd">        If not given, all classes are supposed to have weight one. For</span>
<span class="sd">        multi-output problems, a list of dicts can be provided in the same</span>
<span class="sd">        order as the columns of y.</span>

<span class="sd">        Note that for multioutput (including multilabel) weights should be</span>
<span class="sd">        defined for each class of every column in its own dict. For example,</span>
<span class="sd">        for four-class multilabel classification weights should be</span>
<span class="sd">        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of</span>
<span class="sd">        [{1:1}, {2:5}, {3:1}, {4:1}].</span>

<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``</span>

<span class="sd">        For multi-output, the weights of each column of y will be multiplied.</span>

<span class="sd">        Note that these weights will be multiplied with sample_weight (passed</span>
<span class="sd">        through the fit method) if sample_weight is specified.</span>

<span class="sd">    presort : bool, optional (default=False)</span>
<span class="sd">        Whether to presort the data to speed up the finding of best splits in</span>
<span class="sd">        fitting. For the default settings of a decision tree on large</span>
<span class="sd">        datasets, setting this to true may slow down the training process.</span>
<span class="sd">        When using either a smaller dataset or a restricted depth, this may</span>
<span class="sd">        speed up the training.</span>

<span class="sd">    prune : string, optional (default=None)</span>
<span class="sd">        Determines the pruning strategy. Options are None for no pruning, </span>
<span class="sd">        &#39;rep&#39; for Reduced Error Pruning and &#39;ebp&#39; for Error Based Pruning.</span>

<span class="sd">    rep_val_percentage : float (default=0.1)</span>
<span class="sd">        Determines which percentage of the training set can be used as </span>
<span class="sd">        validation set for Reduced Error Pruning. It must be in the [0.0, 1.0]</span>
<span class="sd">        range. Only valid if `prune=&#39;rep&#39;`.</span>

<span class="sd">    ebp_confidence : float (default=0.25)</span>
<span class="sd">        The confidence value that determines the upper bound on the training error.</span>
<span class="sd">        It must be in the (0.0, 0.5] interval. Only valid if `prune=&#39;ebp&#39;`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    classes_ : array of shape = [n_classes] or a list of such arrays</span>
<span class="sd">        The classes labels (single output problem),</span>
<span class="sd">        or a list of arrays of class labels (multi-output problem).</span>

<span class="sd">    feature_importances_ : array of shape = [n_features]</span>
<span class="sd">        The feature importances. The higher, the more important the</span>
<span class="sd">        feature. The importance of a feature is computed as the (normalized)</span>
<span class="sd">        total reduction of the criterion brought by that feature.  It is also</span>
<span class="sd">        known as the Gini importance [4]_.</span>

<span class="sd">    max_features_ : int,</span>
<span class="sd">        The inferred value of max_features.</span>

<span class="sd">    n_classes_ : int or list</span>
<span class="sd">        The number of classes (for single output problems),</span>
<span class="sd">        or a list containing the number of classes for each</span>
<span class="sd">        output (for multi-output problems).</span>

<span class="sd">    n_features_ : int</span>
<span class="sd">        The number of features when ``fit`` is performed.</span>

<span class="sd">    n_outputs_ : int</span>
<span class="sd">        The number of outputs when ``fit`` is performed.</span>

<span class="sd">    tree_ : Tree object</span>
<span class="sd">        The underlying Tree object.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The original `DecisionTreeClassifier` [1]_ only provided some simple</span>
<span class="sd">    early stopping criteria to limit the size of the induced tree. This </span>
<span class="sd">    `PruneableDecisionTreeClassifier` additionally includes two pruning </span>
<span class="sd">    strategies: Reduced Error Pruning (REP) [3]_ [4]_ and Error Based </span>
<span class="sd">    Pruning (EBP) [2]_. If `prune=None` this class acts like a regular</span>
<span class="sd">    `DecisionTreeClassifier`.</span>

<span class="sd">    The features are always randomly permuted at each split. Therefore,</span>
<span class="sd">    the best found split may vary, even with the same training data and</span>
<span class="sd">    ``max_features=n_features``, if the improvement of the criterion is</span>
<span class="sd">    identical for several splits enumerated during the search of the best</span>
<span class="sd">    split. To obtain a deterministic behaviour during fitting,</span>
<span class="sd">    ``random_state`` has to be fixed.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>

<span class="sd">    .. [1] L. Breiman, J. Friedman, R. Olshen, and C. Stone, &quot;Classification</span>
<span class="sd">           and Regression Trees&quot;, Wadsworth, Belmont, CA, 1984.</span>

<span class="sd">    .. [2] J Ross Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, 1993.</span>

<span class="sd">    .. [3] J. Ross Quinlan. Simplifying decision trees. International journal of</span>
<span class="sd">           man-machine studies, 27(3):221-234, 1987.</span>

<span class="sd">    .. [4] Tapio Elomaa and Matti Kaariainen. An analysis of reduced error</span>
<span class="sd">           pruning. Journal of Artificial Intelligence Research, 15:163-187, 2001.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.datasets import load_iris</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.model_selection import cross_val_score</span>
<span class="sd">    &gt;&gt;&gt; from pruneabletree import PruneableDecisionTreeClassifier</span>
<span class="sd">    &gt;&gt;&gt; clf = PruneableDecisionTreeClassifier(random_state=0, prune=&#39;rep&#39;)</span>
<span class="sd">    &gt;&gt;&gt; iris = load_iris()</span>
<span class="sd">    &gt;&gt;&gt; cross_val_score(clf, iris.data, iris.target, cv=10)</span>
<span class="sd">    ...                             # doctest: +SKIP</span>
<span class="sd">    ...</span>
<span class="sd">    array([1.         0.93333333 1.         0.93333333 0.93333333 0.86666667</span>
<span class="sd">           0.86666667 1.         1.         1.        ])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">criterion</span><span class="o">=</span><span class="s2">&quot;gini&quot;</span><span class="p">,</span>
                 <span class="n">splitter</span><span class="o">=</span><span class="s2">&quot;best&quot;</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">presort</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">prune</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">rep_val_percentage</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">ebp_confidence</span><span class="o">=</span><span class="mf">0.25</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PruneableDecisionTreeClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span>
            <span class="n">splitter</span><span class="o">=</span><span class="n">splitter</span><span class="p">,</span>
            <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span>
            <span class="n">min_samples_split</span><span class="o">=</span><span class="n">min_samples_split</span><span class="p">,</span>
            <span class="n">min_samples_leaf</span><span class="o">=</span><span class="n">min_samples_leaf</span><span class="p">,</span>
            <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
            <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span>
            <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">max_leaf_nodes</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="n">min_impurity_decrease</span><span class="p">,</span>
            <span class="n">min_impurity_split</span><span class="o">=</span><span class="n">min_impurity_split</span><span class="p">,</span>
            <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">,</span>
            <span class="n">presort</span><span class="o">=</span><span class="n">presort</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">prune</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;rep&#39;</span><span class="p">,</span> <span class="s1">&#39;ebp&#39;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`prune` parameter must be either None, &#39;rep&#39; or &#39;ebp&#39;&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prune</span> <span class="o">=</span> <span class="n">prune</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">rep_val_percentage</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`rep_val_percentage` must be a float in [0, 1]&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rep_val_percentage</span> <span class="o">=</span> <span class="n">rep_val_percentage</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;</span> <span class="n">ebp_confidence</span> <span class="o">&lt;=</span> <span class="mf">0.5</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`ebp_confidence` must be a float in (0, 0.5]&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ebp_confidence</span> <span class="o">=</span> <span class="n">ebp_confidence</span>

<div class="viewcode-block" id="PruneableDecisionTreeClassifier.fit"><a class="viewcode-back" href="../../pruneabletree.prune.html#pruneabletree.prune.PruneableDecisionTreeClassifier.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">X_idx_sorted</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Build a decision tree classifier from the training set (X, y).</span>

<span class="sd">        The tree is pruned afterwards using the given pruning strategy.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csc_matrix``.</span>

<span class="sd">        y : array-like, shape = [n_samples] or [n_samples, n_outputs]</span>
<span class="sd">            The target values (class labels) as integers or strings.</span>

<span class="sd">        sample_weight : array-like, shape = [n_samples] or None</span>
<span class="sd">            Sample weights. If None, then samples are equally weighted. Splits</span>
<span class="sd">            that would create child nodes with net zero or negative weight are</span>
<span class="sd">            ignored while searching for a split in each node. Splits are also</span>
<span class="sd">            ignored if they would result in any single class carrying a</span>
<span class="sd">            negative weight in either child node.</span>

<span class="sd">        check_input : boolean, (default=True)</span>
<span class="sd">            Allow to bypass several input checking.</span>
<span class="sd">            Don&#39;t use this parameter unless you know what you do.</span>

<span class="sd">        X_idx_sorted : array-like, shape = [n_samples, n_features], optional</span>
<span class="sd">            The indexes of the sorted training input samples. If many tree</span>
<span class="sd">            are grown on the same dataset, this allows the ordering to be</span>
<span class="sd">            cached between trees. If None, the data will be sorted here.</span>
<span class="sd">            Don&#39;t use this parameter unless you know what to do.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">prune</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">PruneableDecisionTreeClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                <span class="n">check_input</span><span class="o">=</span><span class="n">check_input</span><span class="p">,</span>
                <span class="n">X_idx_sorted</span><span class="o">=</span><span class="n">X_idx_sorted</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">prune</span> <span class="o">==</span> <span class="s1">&#39;rep&#39;</span><span class="p">:</span>
            <span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                <span class="n">test_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rep_val_percentage</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
                <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">PruneableDecisionTreeClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                <span class="n">check_input</span><span class="o">=</span><span class="n">check_input</span><span class="p">,</span>
                <span class="n">X_idx_sorted</span><span class="o">=</span><span class="n">X_idx_sorted</span><span class="p">)</span>
            <span class="n">ReducedErrorPruner</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree_</span><span class="p">)</span><span class="o">.</span><span class="n">prune</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">prune</span> <span class="o">==</span> <span class="s1">&#39;ebp&#39;</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">PruneableDecisionTreeClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                <span class="n">check_input</span><span class="o">=</span><span class="n">check_input</span><span class="p">,</span>
                <span class="n">X_idx_sorted</span><span class="o">=</span><span class="n">X_idx_sorted</span><span class="p">)</span>
            <span class="n">ErrorBasedPruner</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ebp_confidence</span><span class="p">)</span><span class="o">.</span><span class="n">prune</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unknown pruning method: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prune</span><span class="p">))</span>

        <span class="c1"># print(&quot;Tree with {} nodes and {} leaves.&quot;.format(self.n_actual_nodes, self.n_leaves))</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">n_actual_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns the actual number of nodes after pruning.</span>
<span class="sd">        </span>
<span class="sd">        This differs from the `tree_.node_count` property, which contains the </span>
<span class="sd">        number of nodes before pruning. Updating this value would break other </span>
<span class="sd">        features since the underlying data structures still have their original </span>
<span class="sd">        sizes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Pruner</span><span class="o">.</span><span class="n">num_actual_nodes</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree_</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">n_leaves</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns the number of leaves.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Pruner</span><span class="o">.</span><span class="n">num_leaves</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree_</span><span class="p">)</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Sven Van Hove.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'0.1.0',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>

  

  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>